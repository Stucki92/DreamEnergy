{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c719ba1",
   "metadata": {},
   "source": [
    "# Dream Energy : Trafic Analysis around landmarks, based on Coyote databases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40bdc23",
   "metadata": {},
   "source": [
    "The goal of this notebook is to answer : What is the trafic situation around some landmarks ?\n",
    "\n",
    "- Datasets :\n",
    "    * The landmarks are provided by the client. They are composed of a name (=description), a latitude and a longitude.\n",
    "    \n",
    "    * The Coyote dataset. This dataset is present on the ALEIA server 172.16.201.164, in the database coyote.\n",
    "        It contains the following information on the Coyote traces :\n",
    "        - id of the vehicule\n",
    "        - time in seconds since 1st second of 1970\n",
    "        - GPS coordonates (= latitude, longitude)\n",
    "        - speed of the vehicule\n",
    "    \n",
    "    * The journey datatet. This dataset is present on the ALEIA server 172.16.201.133, in the database coyote.\n",
    "        A journey represents a squence of several Coyote traces. It contains the following information :\n",
    "        - id of the vehicule\n",
    "        - duration in seconds of the trip\n",
    "        - distance in km of the trip\n",
    "        - GPS coordonates (= latitude, longitude) of every Coyote points forming the trip\n",
    "        \n",
    "        \n",
    "- Notebooks :\n",
    "    * 1st part :\n",
    "        Extract Coyote and Journey databases into csv format (useful for ALEIA plateform)\n",
    "        Use of Pymongo librairie\n",
    "        \n",
    "    * 2nd part :\n",
    "        For every landmark, count how many vehicules drived close to it.\n",
    "        - Creation of a square around the landmark, based on his latitude, longitude\n",
    "        - Compute distance landmark - Coyote trace, if the trace is in the square around the landmark\n",
    "        \n",
    "    * 3rd part :\n",
    "        For every landmark, compute how many vehicules were traveling for more than a certain a distance (threshold(s) chosen by the client)\n",
    "        - For every vehicules that went closed to a landmark, get his journey info in the journey database\n",
    "        - Compute how many vehicules have a journey distance above the threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc52bac7",
   "metadata": {},
   "source": [
    "### Import librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1688217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pymongo\n",
    "\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import glob\n",
    "import os, shutil\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import csv\n",
    "import glob\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6f1a76",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e84ec676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_coyote(path_minutes,path_hours,col,start_date, end_date,period=60):    \n",
    "    ''' function to extract data from MongoDB Coyote '''\n",
    "    '''\n",
    "    input :\n",
    "    - col = database column\n",
    "    - start_date (second since 1970) = time beginning of the extraction\n",
    "    - end_date (second since 1970) = time end of the extraction\n",
    "     period in second = batch extraction (several small requests rather than one big)\n",
    "    \n",
    "    output :\n",
    "    - creation of Hours repository with csv for every two hours of the day   \n",
    "    - Error if no folder Datasets with two folders Minutes and Hours already created\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #For every period, create a csv with the data\n",
    "    for i in tqdm(range (start_date,end_date,period)):\n",
    "        data_1 = [x['_id'] for x in col.find({'time': {'$gte': i,  '$lt': i+period}})]\n",
    "        data_2 = [x['id'] for x in col.find({'time': {'$gte': i,  '$lt': i+period}})]\n",
    "        data_3 = [x['time'] for x in col.find({'time': {'$gte': i,  '$lt': i+period}})]\n",
    "        data_4 = [x['speed'] for x in col.find({'time': {'$gte': i,  '$lt': i+period}})]\n",
    "\n",
    "        data_longitude = [x['loc']['coordinates'][0] for x in col.find({'time': {'$gte': i,  '$lt': i+period}})]\n",
    "        data_latitude = [x['loc']['coordinates'][1] for x in col.find({'time': {'$gte': i,  '$lt': i+period}})]\n",
    "\n",
    "        data_tuples = list(zip(data_1,data_2,data_3,data_4,data_latitude,data_longitude))\n",
    "        df = pd.DataFrame(data_tuples, columns=['_id','id_cars','time','speed','latitude','longitude'])\n",
    "        df.to_csv(path_minutes + '/' + str(i) + '.csv')\n",
    "        \n",
    "    #Concat every small csv created before in one big (due to the period parameter)\n",
    "    all_files = glob.glob(path_minutes + \"/*.csv\")\n",
    "    li = []\n",
    "    for filename in tqdm(all_files):\n",
    "        df = pd.read_csv(filename, index_col=None, header=0, encoding = 'cp1252')\n",
    "        li.append(df)\n",
    "\n",
    "    frame = pd.concat(li, axis=0, ignore_index=True)\n",
    "    frame.to_csv(path_hours + '/' + str(i) + '.csv')\n",
    "\n",
    "    #Annexe : CLean folder with the small csv files.\n",
    "    for filename in os.listdir(path_minutes):\n",
    "        file_path = os.path.join(path_minutes, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2a86caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dossier_stockage_resultat(noms_points,parent_dir):\n",
    "    '''\n",
    "    Function to create the folder where the results of phase 1 will be saved, before phase 2\n",
    "    input :\n",
    "    - noms_points = list\n",
    "    - parents_dir = folder where to save the ID of the vehicule\n",
    "    \n",
    "    output\n",
    "    - create new folders\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    for nom in noms_points:\n",
    "        path = os.path.join(parent_dir, nom) \n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "            \n",
    "def concat_csv(path,extension):\n",
    "    '''\n",
    "    Function to concat csv files\n",
    "    \n",
    "    input : \n",
    "    - path = path of the csv files to concatenate in one csv\n",
    "    - extension (string) = filter on the end of the name of csv files to concat\n",
    "    \n",
    "    output :\n",
    "    - the dataframe concatenated    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    all_files = glob.glob(path + \"/*\" + extension)\n",
    "    li = []\n",
    "    for filename in all_files:\n",
    "        df = pd.read_csv(filename, index_col=None, header=0, encoding = 'cp1252')\n",
    "        li.append(df)\n",
    "\n",
    "    frame = pd.concat(li, axis=0, ignore_index=True)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f03c6a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance in kilometers between two points on the earth (specified in decimal degrees)\n",
    "    \n",
    "    input :\n",
    "    - lon1, lat1 = decimal degrees cordinates of first points\n",
    "    - lon2, lat2 = decimal degrees cordinates of second points\n",
    "    \n",
    "    output :\n",
    "    -   the distance between the two points\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # convert decimal degrees to radians \n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    r = 6372.8 # Radius of earth in kilometers. \n",
    "    \n",
    "    return c * r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b58bd6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparaison_distance_methode_carre(latitude_ref,longitude_ref,latitude_car,longitude_car,radius):\n",
    "    '''\n",
    "    Function to check if a car is in a given radius around the landmark\n",
    "\n",
    "    input :\n",
    "    - latitude_ref, longitude_ref = ref point coordinates\n",
    "    - latitude, longitude = cars gps coordinates\n",
    "    - radius = limit radius of cars counting in km\n",
    "    \n",
    "    return\n",
    "     - 1 if the car is in the radius\n",
    "     - 0 otherwise  \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if (haversine(longitude_car, latitude_car, longitude_ref, latitude_ref) <= radius):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70f0bbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance_trip(trip):\n",
    "    '''\n",
    "    TODO\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    distance_km = 0\n",
    "    \n",
    "    for i in range(1,len(trip)):\n",
    "        \n",
    "        longitude_1 = trip[i-1]['coordinates'][0]\n",
    "        longitude_2 = trip[i]['coordinates'][0]\n",
    "        latitude_1 = trip[i-1]['coordinates'][1]    \n",
    "        latitude_2 = trip[i]['coordinates'][1]\n",
    "    \n",
    "        distance_km = distance_km + haversine(longitude_1, latitude_1, longitude_2, latitude_2)\n",
    "        \n",
    "    return round(distance_km,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3055a457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_column_counter(source, radius, threshold, isabove = True):\n",
    "    '''\n",
    "    Function to compute how many cars, that drove within a certain radius around the landmark, travelled for a distance above a threshold (= compute long trips)\n",
    "    \n",
    "    input :\n",
    "    - source : folder with distance csv files for every landmark\n",
    "    - radius : radius around the landmark\n",
    "    - threshold : distance threshold of the trip in km\n",
    "    \n",
    "    output : \n",
    "    - two columns dataframe :\n",
    "        - landmarks name/description\n",
    "        - number of vehicule with a travelled distance superior to the threshold     \n",
    "    \n",
    "    '''\n",
    "    count = []\n",
    "    name = []\n",
    "    all_files = glob.glob(source + \"/*\" + str(radius) + \"km*\")\n",
    "    \n",
    "    for filename in all_files:\n",
    "        dataframe = pd.read_csv(filename, encoding = 'cp1252')\n",
    "        name.append(filename[75:-4])\n",
    "        if (isabove):\n",
    "            count.append(dataframe[dataframe.distance >= threshold].shape[0])\n",
    "            title = 'nombre_vehicule_' + str(radius) + 'km_distance_sup_seuil_' + str(threshold) + 'km'\n",
    "        else:\n",
    "            count.append(dataframe[dataframe.distance < threshold].shape[0])\n",
    "            title = 'nombre_vehicule_' + str(radius) + 'km_distance_inf_seuil_' + str(threshold) + 'km'\n",
    "\n",
    "    label = np.array(count)  \n",
    "    \n",
    "    dataset = pd.DataFrame({'description': name, title : label}, columns = ['description', title])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db344740",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Phase 1 : Count cars near the landmarks\n",
    "\n",
    "def nombre_vehicule_proche(dataframe, dataframe_points, titre_csv, km_autour_du_point, path):\n",
    "    \n",
    "    \n",
    "    latitude_points = np.array(dataframe_points.coord_1)\n",
    "    longitude_points = np.array(dataframe_points.coord_2)\n",
    "    noms_points = np.array(dataframe_points.description) # A modifier selon le cas\n",
    "    \n",
    "    N = dataframe_points.shape[0]\n",
    "    dossier_stockage_resultat(noms_points,path)\n",
    "    \n",
    "    latitude = np.array(dataframe.latitude)\n",
    "    longitude = np.array(dataframe.longitude)\n",
    "    \n",
    "    tab_inter_latitude = np.tile(np.array(latitude), (N, 1))\n",
    "    tab_inter_longitude = np.tile(np.array(longitude), (N, 1))\n",
    "\n",
    "    for j in km_autour_du_point:\n",
    "        \n",
    "        cond_1 = tab_inter_latitude[np.absolute(tab_inter_latitude - latitude_points.reshape(N,1)) <= j/111]\n",
    "        cond_2 = tab_inter_longitude[(np.absolute(tab_inter_longitude - longitude_points.reshape(N,1))) <= j/85]\n",
    "        dataframe_int = dataframe[dataframe.latitude.isin(cond_1) & dataframe.longitude.isin(cond_2)]\n",
    "                              \n",
    "        for i in range(N):\n",
    "            latitude_int = np.array(dataframe_int.latitude)\n",
    "            longitude_int = np.array(dataframe_int.longitude)\n",
    "    \n",
    "            cond_3 = latitude_int[(np.absolute(latitude_int - latitude_points[i])) <= j/111]\n",
    "            cond_4 = longitude_int[(np.absolute(longitude_int - longitude_points[i])) <= j/85]\n",
    "            dataframe_int_2 = dataframe_int[dataframe_int.latitude.isin(cond_3) & dataframe_int.longitude.isin(cond_4)]\n",
    "            dataframe_int_2['isclose'] = 0\n",
    "\n",
    "            dataframe_int_2['isclose'] = dataframe_int_2.apply(lambda x: comparaison_distance_methode_carre(latitude_points[i],longitude_points[i],x['latitude'],x['longitude'],j),axis=1)\n",
    "            dataframe_int_3 = dataframe_int_2[dataframe_int_2['isclose'] == 1][['_id','time','id_cars','latitude','longitude']]\n",
    "            \n",
    "            dataframe_int_3.groupby('id_cars').first().to_csv(path + '/'  + str(noms_points[i]) + '/' + titre_csv + '_' + str(j) + 'km.csv', encoding = 'cp1252')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efa7168",
   "metadata": {},
   "source": [
    "#Phase 2 : Compute cars trips distance for every vehicules close to landmarks\n",
    "\n",
    "def distance_vehicule_trajet(source,col_trajets,titre,path_distance):\n",
    "    dataframe = pd.read_csv(source, encoding = 'cp1252')\n",
    "\n",
    "    taille = dataframe.shape[0]\n",
    "    dataframe['distance'] = np.zeros(taille)\n",
    "    \n",
    "    data = [x for x in col_trajets.find({'id': {'$in': dataframe.id_cars.to_list()}})]\n",
    "    df = pd.DataFrame(data)         \n",
    "           \n",
    "    if (df.shape[0] == 0):\n",
    "        return 0\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        for identifiant in dataframe['id_cars']: #Pour dataframe \n",
    "            df_inter = df[df['id'] == identifiant].reset_index()\n",
    "            N = df_inter.shape[0]\n",
    "            if (N == 0):\n",
    "                dataframe.loc[dataframe.id_cars == identifiant, 'distance'] = 0\n",
    "            elif (N == 1):\n",
    "                dataframe.loc[dataframe.id_cars == identifiant, 'distance'] = df_inter['trip_distance_km'][0]\n",
    "            else :\n",
    "                for j in range(N): #Pour les trajets présents \n",
    "                    distance_off = 0\n",
    "                    if ((df_inter['begin'][j]['time'] <  dataframe.loc[dataframe.id_cars == identifiant]['time'].iloc[0]) & (dataframe.loc[dataframe.id_cars == identifiant]['time'].iloc[0] - 3*3600 < df_inter['begin'][j]['time'])):\n",
    "                        distance_off = df_inter['trip_distance_km'][j]\n",
    "                        dataframe.loc[dataframe.id_cars == identifiant, 'distance'] = distance_off\n",
    "                        break\n",
    "                \n",
    "        n_vehicule_trajet = dataframe[dataframe['distance'] > 0]\n",
    "    \n",
    "        n_vehicule_sans_trajet = dataframe[dataframe['distance'] == 0].shape[0]\n",
    "    \n",
    "        n_vehicule_trajet.to_csv(path_distance + '/' + titre + '.csv', encoding = 'cp1252')\n",
    "        return n_vehicule_sans_trajet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "495cb063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_vehicule_trajet(source,col_trajets,titre,path_distance):\n",
    "    dataframe = pd.read_csv(source, encoding = 'cp1252')\n",
    "\n",
    "    taille = dataframe.shape[0]\n",
    "    dataframe['distance'] = np.zeros(taille)\n",
    "\n",
    "    data = [x for x in col_trajets.find({'id': {'$in': dataframe.id_cars.to_list()}})]\n",
    "    df = pd.DataFrame(data) \n",
    "    \n",
    "    for identifiant in dataframe['id_cars']: \n",
    "        \n",
    "        #Creer un dataframe intermediaire avec uniquement les trajets de ce véhicule\n",
    "        df_inter = df[df['id'] == identifiant].reset_index()\n",
    "        N = df_inter.shape[0]\n",
    "        \n",
    "        #Si le véhicule possède 1 ou plusieurs trajets :\n",
    "        if (N != 0):\n",
    "            for j in range(N):\n",
    "            \n",
    "                #On vérifie que t(entré véhicule dans zone proche) est inf à t(fin de trajet) et sup à t(début de trajet)\n",
    "                if ((df_inter['begin'][j]['time'] <  dataframe.loc[dataframe.id_cars == identifiant]['time'].iloc[0]) & (df_inter['end'][j]['time'] > dataframe.loc[dataframe.id_cars == identifiant]['time'].iloc[0])):\n",
    "                \n",
    "                #Si plusieurs trajets, prendre celui ou il y a le points GPS d'entrée de zone\n",
    "                    trips_points = df_inter['loc'].iloc[j]\n",
    "                    latitude_point_zone = dataframe.loc[dataframe.id_cars == identifiant]['latitude'].iloc[0]\n",
    "                    longitude_point_zone = dataframe.loc[dataframe.id_cars == identifiant]['longitude'].iloc[0]\n",
    "               \n",
    "                    for k in range(len(trips_points)):\n",
    "                        if (trips_points[k]['coordinates'] == [longitude_point_zone, latitude_point_zone]):                      \n",
    "                            trip = trips_points[:k+1]\n",
    "                            dataframe.loc[dataframe.id_cars == identifiant, 'distance'] = compute_distance_trip(trip)\n",
    "                            break\n",
    "                    break\n",
    "                                                \n",
    "                #Si le points GPS d'entrée de zone apparait plusieurs fois, prendre celui dont t est le plus faible (1er point d'entrer)\n",
    "                #Si aucun trajet avec le points GPS d'entrée de zone, retourner 0 en distance\n",
    "                \n",
    "                \n",
    "    n_vehicule_trajet = dataframe[dataframe['distance'] > 0]\n",
    "    n_vehicule_trajet.to_csv(path_distance + '/' + titre + '.csv', encoding = 'cp1252')\n",
    "        \n",
    "    n_vehicule_sans_trajet = dataframe[dataframe['distance'] == 0].shape[0]        \n",
    "    return round(n_vehicule_sans_trajet / dataframe.shape[0] * 100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de8becfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Phase 3 : Creation of the final csv with phase 1 and 2 results\n",
    "\n",
    "def compte_rendu_csv(source_distance,path_result,file,threshold_1,threshold_2,radius):\n",
    "    \n",
    "    data_rendu = pd.read_csv(path_result + '/' + file + '/carre_rendu_final.csv', encoding = 'cp1252')\n",
    "    \n",
    "    for j in radius:\n",
    "        data_trajets_manquants = pd.read_csv(path_result + '/' + file + '/n_vehicule_sans_trajets_' + str(j) + 'km.csv', encoding = 'cp1252')\n",
    "\n",
    "        dataset_0 = add_column_counter(source_distance,j,threshold_1, isabove = False)\n",
    "        data_rendu = data_rendu.merge(dataset_0, how='left', on='description')\n",
    "        \n",
    "        dataset_1 = add_column_counter(source_distance,j,threshold_1)\n",
    "        data_rendu = data_rendu.merge(dataset_1, how='left', on='description')\n",
    "        \n",
    "        dataset_2 = add_column_counter(source_distance,j,threshold_2)\n",
    "        data_rendu = data_rendu.merge(dataset_2, how='left', on='description')\n",
    "        \n",
    "        data_rendu['nombre_vehicule_' + str(j) + 'km_distance_seuil_' + str(threshold_1) + '_' + str(threshold_2) + 'km'] = data_rendu['nombre_vehicule_' + str(j) + 'km_distance_sup_seuil_' + str(threshold_1) + 'km'] - data_rendu['nombre_vehicule_' + str(j) + 'km_distance_sup_seuil_' + str(threshold_2) + 'km']\n",
    "        data_rendu = data_rendu.drop(['nombre_vehicule_' + str(j) + 'km_distance_sup_seuil_' + str(threshold_1) + 'km'], axis=1)\n",
    "        \n",
    "        data_rendu = data_rendu.merge(data_trajets_manquants[['description','pourcentage_vehicules_trajets_manquants_' + str(j) + 'km']], how='left', on='description')\n",
    "        \n",
    "    return data_rendu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52cb6103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(source_client, source_gps, path_rendu, path_inter, path_distance, km_autour_du_point, col_trajets, name,seuil_a,seuil_b):\n",
    "    \n",
    "    #Remove each file in the ID_cars folder. Folder has to be empty for next steps to proceed\n",
    "    folderlist = glob.glob(os.path.join(path_inter, \"*\"))\n",
    "    for f in folderlist:\n",
    "        filelist = glob.glob(os.path.join(f, \"*\"))\n",
    "        for file in filelist:\n",
    "            os.remove(file)\n",
    "        os.rmdir(f)\n",
    "\n",
    "    #Import data (ref)\n",
    "    dataframe_client = pd.read_csv(source_client, encoding = 'cp1252')\n",
    "    #List all files in the gps folder (here : the 2 hours csv files with the data from the coyote database)\n",
    "    onlyfiles = [f for f in os.listdir(source_gps) if os.path.isfile(os.path.join(source_gps, f))]\n",
    "    \n",
    "    \n",
    "    #Phase 1 (1st part)\n",
    "    start_time = time.time()\n",
    "    for file in tqdm(onlyfiles): # CSV gps (2H)\n",
    "        dataframe = pd.read_csv(source_gps + file, encoding = 'cp1252')\n",
    "        nombre_vehicule_proche(dataframe,dataframe_client,str(file[:10]),km_autour_du_point,path_inter)\n",
    "        \n",
    "    print(\"--- Compteur Véhicule par heure : %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "    #Final Dataframe creation\n",
    "    dataframe_final = dataframe_client\n",
    "    \n",
    "    directory_contents = os.listdir(path_inter)\n",
    "\n",
    "    #Phase 1 (2nd part)\n",
    "    for j in km_autour_du_point: #2 rayon de reparage\n",
    "        dataframe_final['n_vehicules_rayon_' + str(j) +'km'] = np.zeros(dataframe_final.shape[0])\n",
    "        for k in directory_contents:\n",
    "            dataframe_km = concat_csv(path_inter + '/' + k, str(j) + 'km.csv')\n",
    "            dataframe_final.loc[dataframe_final.description == k, 'n_vehicules_rayon_' + str(j) +'km'] = dataframe_km['id_cars'].unique().size\n",
    "            dataframe_km.groupby('id_cars').first().to_csv(path_inter + '/'  + str(k) + '/' + 'all_day_' + str(j) + 'km.csv', encoding = 'cp1252')\n",
    "                \n",
    "\n",
    "    dataframe_final.to_csv(path_rendu + '/' + str(name) + '/carre_rendu_final.csv', index = False, encoding = 'cp1252')\n",
    "    \n",
    "    #Remove each file in the Distance folder. Folder has to be empty for next steps to proceed\n",
    "    filelist = glob.glob(os.path.join(path_distance, \"*\"))\n",
    "    for f in filelist:\n",
    "        os.remove(f)\n",
    "\n",
    "    #Phase 2    \n",
    "    start_time = time.time()\n",
    "    for j in km_autour_du_point:\n",
    "        prop_vehicules_sans_trajets = []\n",
    "        for i in tqdm(directory_contents):\n",
    "            path_km = path_inter + '/' + i + '/all_day_' + str(j) + 'km.csv'\n",
    "            n_vehicules = distance_vehicule_trajet(path_km, col_trajets,'client_' + str(j) + 'km_' + i,path_distance)\n",
    "            prop_vehicules_sans_trajets.append(n_vehicules)\n",
    "            \n",
    "        pd.DataFrame({'description': directory_contents, 'pourcentage_vehicules_trajets_manquants_' + str(j) + 'km' : prop_vehicules_sans_trajets}, columns = ['description', 'pourcentage_vehicules_trajets_manquants_' + str(j) + 'km']).to_csv(path_rendu + '/' + str(name) + '/n_vehicule_sans_trajets_' + str(j) + 'km.csv', encoding = 'cp1252')\n",
    "        \n",
    "    print(\"--- Distribution trajet : %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "    #Phase 3\n",
    "    \n",
    "    data_rendu_final = compte_rendu_csv(path_distance,path_rendu,name,seuil_a,seuil_b,km_autour_du_point)\n",
    "    data_rendu_final.to_csv(path_rendu + '/' + str(name) + '/rendu_final.csv', encoding = 'cp1252')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a46752a",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "784b4227",
   "metadata": {},
   "outputs": [],
   "source": [
    "myclient = pymongo.MongoClient(\"mongodb://172.16.201.164:27017/\")\n",
    "mydb = myclient[\"coyote\"]\n",
    "mycol = mydb[\"coyote_data_20200113\"]\n",
    "\n",
    "myclient_trajets = pymongo.MongoClient(\"mongodb://172.16.201.133:27017/\")\n",
    "mydb_trajets = myclient_trajets[\"coyote\"]\n",
    "mycol_trajets = mydb_trajets[\"trajet_20200113\"]\n",
    "\n",
    "path_gps = 'C:/Users/NicolasSTUCKI/Documents/Dream Energy/Datasets/Hours_plateforme_format/'\n",
    "path_minutes = 'C:/Users/NicolasSTUCKI/Documents/Dream Energy/Datasets/Minutes/'\n",
    "\n",
    "start = datetime.datetime(2020,1,14,0).timestamp()\n",
    "end = datetime.datetime(2020,1,15,0).timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66b96623",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(int(start),int(end),7200):\n",
    "    #get_data_coyote(path_minutes,path_gps,mycol,i,i+7200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c621b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_repere = 'C:/Users/NicolasSTUCKI/Documents/Dream Energy/Datasets/Repere'\n",
    "path_rendu = 'C:/Users/NicolasSTUCKI/Documents/Dream Energy/Datasets/Rendu'\n",
    "path_inter = 'C:/Users/NicolasSTUCKI/Documents/Dream Energy/Datasets/Id_voiture'\n",
    "path_distance = 'C:/Users/NicolasSTUCKI/Documents/Dream Energy/Datasets/Distance'\n",
    "\n",
    "radius_list = [3,5]\n",
    "seuil_1 = 40\n",
    "seuil_2 = 150\n",
    "\n",
    "filesrepere = [f[:-4] for f in listdir(path_repere) if isfile(join(path_repere, f))]\n",
    "dossier_stockage_resultat(filesrepere,path_rendu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5afc8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]\n",
      "  0%|                                                                                           | 0/19 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|████▎                                                                              | 1/19 [00:01<00:33,  1.88s/it]\u001b[A\n",
      " 11%|████████▋                                                                          | 2/19 [00:03<00:29,  1.76s/it]\u001b[A\n",
      " 16%|█████████████                                                                      | 3/19 [00:08<00:49,  3.12s/it]\u001b[A\n",
      " 21%|█████████████████▍                                                                 | 4/19 [00:31<02:43, 10.90s/it]\u001b[A\n",
      " 26%|█████████████████████▊                                                             | 5/19 [00:34<01:53,  8.11s/it]\u001b[A\n",
      " 32%|██████████████████████████▏                                                        | 6/19 [00:57<02:52, 13.27s/it]\u001b[A\n",
      " 37%|██████████████████████████████▌                                                    | 7/19 [01:15<02:58, 14.86s/it]\u001b[A\n",
      " 42%|██████████████████████████████████▉                                                | 8/19 [01:39<03:13, 17.60s/it]\u001b[A\n",
      " 47%|███████████████████████████████████████▎                                           | 9/19 [01:51<02:38, 15.85s/it]\u001b[A\n",
      " 53%|███████████████████████████████████████████▏                                      | 10/19 [02:14<02:44, 18.24s/it]\u001b[A\n",
      " 58%|███████████████████████████████████████████████▍                                  | 11/19 [02:24<02:04, 15.55s/it]\u001b[A\n",
      " 63%|███████████████████████████████████████████████████▊                              | 12/19 [02:48<02:06, 18.06s/it]\u001b[A\n",
      " 68%|████████████████████████████████████████████████████████                          | 13/19 [02:59<01:36, 16.07s/it]\u001b[A\n",
      " 74%|████████████████████████████████████████████████████████████▍                     | 14/19 [03:23<01:31, 18.36s/it]\u001b[A\n",
      " 79%|████████████████████████████████████████████████████████████████▋                 | 15/19 [03:41<01:13, 18.28s/it]\u001b[A\n",
      " 84%|█████████████████████████████████████████████████████████████████████             | 16/19 [04:04<00:59, 19.91s/it]\u001b[A\n",
      " 89%|█████████████████████████████████████████████████████████████████████████▎        | 17/19 [04:15<00:34, 17.13s/it]\u001b[A\n",
      " 95%|█████████████████████████████████████████████████████████████████████████████▋    | 18/19 [04:26<00:15, 15.38s/it]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [04:31<00:00, 14.28s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Compteur Véhicule par heure : 271.4006609916687 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                           | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|████▌                                                                              | 1/18 [00:09<02:47,  9.88s/it]\u001b[A\n",
      " 11%|█████████▏                                                                         | 2/18 [00:18<02:29,  9.37s/it]\u001b[A\n",
      " 17%|█████████████▊                                                                     | 3/18 [00:24<01:51,  7.43s/it]\u001b[A\n",
      " 22%|██████████████████▍                                                                | 4/18 [00:42<02:46, 11.86s/it]\u001b[A\n",
      " 28%|███████████████████████                                                            | 5/18 [00:45<01:50,  8.53s/it]\u001b[A\n",
      " 33%|███████████████████████████▋                                                       | 6/18 [00:55<01:48,  9.07s/it]\u001b[A\n",
      " 39%|████████████████████████████████▎                                                  | 7/18 [01:12<02:07, 11.57s/it]\u001b[A\n",
      " 44%|████████████████████████████████████▉                                              | 8/18 [01:19<01:41, 10.10s/it]\u001b[A\n",
      " 50%|█████████████████████████████████████████▌                                         | 9/18 [01:39<01:59, 13.31s/it]\u001b[A\n",
      " 56%|█████████████████████████████████████████████▌                                    | 10/18 [01:52<01:45, 13.13s/it]\u001b[A\n",
      " 61%|██████████████████████████████████████████████████                                | 11/18 [01:55<01:10, 10.03s/it]\u001b[A\n",
      " 67%|██████████████████████████████████████████████████████▋                           | 12/18 [02:06<01:02, 10.40s/it]\u001b[A\n",
      " 72%|███████████████████████████████████████████████████████████▏                      | 13/18 [02:12<00:44,  9.00s/it]\u001b[A\n",
      " 78%|███████████████████████████████████████████████████████████████▊                  | 14/18 [02:20<00:35,  8.78s/it]\u001b[A\n",
      " 83%|████████████████████████████████████████████████████████████████████▎             | 15/18 [02:23<00:21,  7.00s/it]\u001b[A\n",
      " 89%|████████████████████████████████████████████████████████████████████████▉         | 16/18 [02:28<00:12,  6.43s/it]\u001b[A\n",
      " 94%|█████████████████████████████████████████████████████████████████████████████▍    | 17/18 [02:37<00:07,  7.31s/it]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 18/18 [02:45<00:00,  9.21s/it]\u001b[A\n",
      "\n",
      "  0%|                                                                                           | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|████▌                                                                              | 1/18 [00:33<09:22, 33.07s/it]\u001b[A\n",
      " 11%|█████████▏                                                                         | 2/18 [00:43<05:18, 19.88s/it]\u001b[A\n",
      " 17%|█████████████▊                                                                     | 3/18 [00:49<03:18, 13.23s/it]\u001b[A\n",
      " 22%|██████████████████▍                                                                | 4/18 [01:16<04:23, 18.83s/it]\u001b[A\n",
      " 28%|███████████████████████                                                            | 5/18 [01:19<02:51, 13.23s/it]\u001b[A\n",
      " 33%|███████████████████████████▋                                                       | 6/18 [01:31<02:33, 12.81s/it]\u001b[A\n",
      " 39%|████████████████████████████████▎                                                  | 7/18 [01:49<02:38, 14.43s/it]\u001b[A\n",
      " 44%|████████████████████████████████████▉                                              | 8/18 [02:00<02:12, 13.26s/it]\u001b[A\n",
      " 50%|█████████████████████████████████████████▌                                         | 9/18 [02:23<02:28, 16.52s/it]\u001b[A\n",
      " 56%|█████████████████████████████████████████████▌                                    | 10/18 [02:46<02:28, 18.54s/it]\u001b[A\n",
      " 61%|██████████████████████████████████████████████████                                | 11/18 [02:50<01:38, 14.03s/it]\u001b[A\n",
      " 67%|██████████████████████████████████████████████████████▋                           | 12/18 [03:04<01:23, 13.86s/it]\u001b[A\n",
      " 72%|███████████████████████████████████████████████████████████▏                      | 13/18 [03:11<00:59, 11.93s/it]\u001b[A\n",
      " 78%|███████████████████████████████████████████████████████████████▊                  | 14/18 [03:21<00:44, 11.25s/it]\u001b[A\n",
      " 83%|████████████████████████████████████████████████████████████████████▎             | 15/18 [03:24<00:26,  8.87s/it]\u001b[A\n",
      " 89%|████████████████████████████████████████████████████████████████████████▉         | 16/18 [03:30<00:15,  7.95s/it]\u001b[A\n",
      " 94%|█████████████████████████████████████████████████████████████████████████████▍    | 17/18 [03:40<00:08,  8.54s/it]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 18/18 [03:49<00:00, 12.76s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Distribution trajet : 395.45874977111816 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████▌                                         | 1/2 [11:09<11:09, 669.82s/it]\n",
      "  0%|                                                                                           | 0/19 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|████▎                                                                              | 1/19 [00:01<00:25,  1.44s/it]\u001b[A\n",
      " 11%|████████▋                                                                          | 2/19 [00:02<00:24,  1.42s/it]\u001b[A\n",
      " 16%|█████████████                                                                      | 3/19 [00:06<00:41,  2.60s/it]\u001b[A\n",
      " 21%|█████████████████▍                                                                 | 4/19 [00:26<02:20,  9.38s/it]\u001b[A\n",
      " 26%|█████████████████████▊                                                             | 5/19 [00:29<01:37,  6.99s/it]\u001b[A\n",
      " 32%|██████████████████████████▏                                                        | 6/19 [00:50<02:33, 11.84s/it]\u001b[A\n",
      " 37%|██████████████████████████████▌                                                    | 7/19 [01:06<02:37, 13.16s/it]\u001b[A\n",
      " 42%|██████████████████████████████████▉                                                | 8/19 [01:27<02:51, 15.63s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "for filerepere in tqdm(filesrepere):\n",
    "    repere = path_repere + '/' + str(filerepere) + '.csv'\n",
    "    main(repere, path_gps, path_rendu, path_inter, path_distance, radius_list, mycol_trajets,filerepere,seuil_1,seuil_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd810f61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77667f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a99b85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.read_csv('./Datasets/Repere/Terrains_DE_cleaned_p2.csv', sep = ',', encoding = 'cp1252')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ddcdc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bafbf5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
